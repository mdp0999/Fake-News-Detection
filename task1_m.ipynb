{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 17:44:11.609388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/rohit/.cache/huggingface/token\n",
      "Login successful\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at bytesizedllm/MalayalamXLM_Roberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.8938223938223938, 'recall': 0.904296875, 'f1-score': 0.8990291262135922, 'support': 512}, '1': {'precision': 0.9021956087824351, 'recall': 0.8915187376725838, 'f1-score': 0.8968253968253967, 'support': 507}, 'accuracy': 0.8979391560353287, 'macro avg': {'precision': 0.8980090013024145, 'recall': 0.897907806336292, 'f1-score': 0.8979272615194944, 'support': 1019}, 'weighted avg': {'precision': 0.897988458576801, 'recall': 0.8979391560353287, 'f1-score': 0.8979326681176009, 'support': 1019}}\n",
      "Validation Macro F1-Score: 0.8979\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90       512\n",
      "           1       0.90      0.89      0.90       507\n",
      "\n",
      "    accuracy                           0.90      1019\n",
      "   macro avg       0.90      0.90      0.90      1019\n",
      "weighted avg       0.90      0.90      0.90      1019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup, AutoModelForSequenceClassification, Adafactor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math, string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # This regex pattern matches most emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "        \"\\U00002500-\\U00002BEF\"  # Chinese characters\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U00002702-\\U000027B0\"  # Additional symbols\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols and pictographs\n",
    "        \"\\U0001F910-\\U0001F9FF\"  # Supplemental symbols and pictographs continued\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', ' ', text)\n",
    "\n",
    "punctuation_list = [r'\\.', r',', r'\\?', r'!', r':', r';', r'\"', r'\\-', r'â€“', r'\\(', r'\\)', r'\\[', r'\\]', r'\\{', r'\\}', r'\\.\\.\\.', r'\\/', r'\\\\', r'@', r'&', r'\\*', r'#', r'%', r'_', r'~', r'`', r'\\^', r'\\|', r'=', r'<', r'>', r'\\+']\n",
    "# Create the regex pattern to match any of the punctuation marks\n",
    "punctuation_pattern = r'(' + '|'.join(punctuation_list) + r')'\n",
    "\n",
    "# Function to clean and tokenize input text\n",
    "def preprocess_text(text):\n",
    "    text = text.replace(\"&amp;\",\" \").replace(\"<br>\",\" \").replace(\"&#39;\",\"'\")\n",
    "    text = remove_html_tags(text)\n",
    "#     url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "#     text = re.sub(url_pattern, '', text)\n",
    "#     text = re.sub(r\".com$\", \"\", text)\n",
    "#     text = re.sub(r\"@\\S+\", \"\", text)\n",
    "#     text = re.sub(r'\\n+',\"\\n\",text)\n",
    "#     text = remove_emojis(text)\n",
    "#     text = re.sub(r'\\d+', ' ', text)\n",
    "#     text = re.sub(r\" @ \", \" \", text)\n",
    "#     text = re.sub(r\" # \", \" \", text)\n",
    "#     text = re.sub(punctuation_pattern, r' \\1 ', text)\n",
    "#     text = remove_punctuation(text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "# Define if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class TransformerXLMRobertaClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 xlm_model_name: str,\n",
    "                 num_labels: int,\n",
    "                 d_model: int = 768,\n",
    "                 nhead: int = 8,\n",
    "                 num_encoder_layers: int = 3,\n",
    "                 num_decoder_layers: int = 3,\n",
    "                 dim_feedforward: int = 2048,\n",
    "                 dropout_prob: float = 0.3):\n",
    "        super(TransformerXLMRobertaClassifier, self).__init__()\n",
    "\n",
    "        # Load XLM-RoBERTa model\n",
    "        self.roberta = AutoModel.from_pretrained(xlm_model_name, cache_dir=\"xlm_roberta1/\")\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, \n",
    "                                                   nhead=nhead, \n",
    "                                                   dim_feedforward=dim_feedforward, \n",
    "                                                   dropout=dropout_prob)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Transformer Decoder Layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, \n",
    "                                                   nhead=nhead, \n",
    "                                                   dim_feedforward=dim_feedforward, \n",
    "                                                   dropout=dropout_prob)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        # Linear layer to map XLM-RoBERTa output to transformer dimension\n",
    "        self.input_projection = nn.Linear(768, d_model)\n",
    "\n",
    "        # Output Classification Layer\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        self.gradient_clip_val = 1.0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get embeddings from XLM-RoBERTa\n",
    "        roberta_outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = roberta_outputs.last_hidden_state\n",
    "\n",
    "        # Project embeddings to match Transformer dimensions\n",
    "        embeddings = self.input_projection(embeddings)\n",
    "\n",
    "        # Create a source mask for the Transformer\n",
    "        seq_len = embeddings.size(1)\n",
    "        src_mask = self._generate_square_subsequent_mask(seq_len).to(embeddings.device)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_output = self.encoder(embeddings.permute(1, 0, 2), src_mask)\n",
    "\n",
    "        # Dummy target input for the Transformer Decoder\n",
    "        # Here, we use the same encoder output for simplicity\n",
    "        tgt = encoder_output.clone()\n",
    "        tgt_mask = self._generate_square_subsequent_mask(seq_len).to(encoder_output.device)\n",
    "\n",
    "        # Transformer Decoder\n",
    "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
    "\n",
    "        # Take the output of the last token\n",
    "        output = decoder_output.permute(1, 0, 2).mean(dim=1)\n",
    "\n",
    "        # Dropout and Classification\n",
    "        output = self.dropout(output)\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        # Calculate loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        \"\"\"Generate a square mask for the sequence to prevent attention to future tokens.\"\"\"\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
    "        return mask\n",
    "    \n",
    "# Load tokenizer and model\n",
    "model_name = \"bytesizedllm/MalayalamXLM_Roberta\"\n",
    "num_labels = 2\n",
    "\n",
    "label2id = {'original': 0, 'Fake': 1}\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bytesizedllm/MalayalamXLM_Roberta\", cache_dir=\"xlm_roberta1/\")\n",
    "\n",
    "model = TransformerXLMRobertaClassifier(model_name, num_labels)\n",
    "best_model_path = \"best_model1.pth\"\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_label(text):\n",
    "    with torch.no_grad():\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move input to device\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pred = torch.argmax(logits, dim=1).cpu().item()  # Get the predicted label\n",
    "    return pred\n",
    "\n",
    "# Load the test dataset\n",
    "test_path = \"old_data/Fake_test_with_labels.csv\"  # Path to the test dataset\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "true_predictions = []\n",
    "predicted = []\n",
    "for text, label in zip(test_df[\"text\"],test_df[\"label\"]):\n",
    "    pred = predict_label(preprocess_text(text))\n",
    "    predicted.append(pred)\n",
    "    true_predictions.append(label2id[label])\n",
    "\n",
    "\n",
    "report = classification_report(true_predictions, predicted)\n",
    "report1 = classification_report(true_predictions, predicted, output_dict=True)\n",
    "macro_f1 = report1['macro avg']['f1-score']\n",
    "print(report1)\n",
    "\n",
    "print(f\"Validation Macro F1-Score: {macro_f1:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_path = \"Fake_test_without_labels.csv\"  # Path to the test dataset\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "\n",
    "data = []\n",
    "for text, ID in zip(test_df[\"text\"],test_df[\"Id\"]):\n",
    "    pred = predict_label(preprocess_text(text))\n",
    "    pred = id2label[pred]\n",
    "    data.append([ID, pred])\n",
    "\n",
    "output_df = pd.DataFrame(data, columns = [\"Id\", \"Labels\"])\n",
    "    \n",
    "\n",
    "    \n",
    "output_df.to_csv(\"byteSizedLLM_Malayalam_task1_run3.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to prediction.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digi",
   "language": "python",
   "name": "digi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
