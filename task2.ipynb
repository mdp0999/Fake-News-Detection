{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476fd235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 14:59:49.596681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/rohit/.cache/huggingface/token\n",
      "Login successful\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of labels:  4 [0, 1, 2, 3] [0, 1, 2, 3] {'FALSE': 0, 'HALF TRUE': 1, 'PARTLY FALSE': 2, 'MOSTLY FALSE': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at bytesizedllm/MalayalamXLM_Roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train text: വിഴിഞ്ഞത്ത് തീരദേശവാസികള്‍ ആക്രമിച്ചപ്പോള്‍ മുഖ്യമന്ത്രി പിണറായി വിജയന്‍ ഉപേക്ഷിച്ച കാര്‍.\n",
      "val text: സ്നേഹത്തില്‍ പൊതിഞ്ഞ പാവക്കുട്ടികള്‍, ഈ സമ്മാനമെല്ലാം പലസ്‌തീനിലെ കുട്ടികള്‍ക്കോ?\n",
      "Epoch 1/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.1750: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:46<00:00,  5.07it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 24.97it/s]\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8942\n",
      "Validation Accuracy: 0.5960\n",
      "Validation Macro F1-Score: 0.1867\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75       149\n",
      "           1       0.00      0.00      0.00        24\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00        63\n",
      "\n",
      "    accuracy                           0.60       250\n",
      "   macro avg       0.15      0.25      0.19       250\n",
      "weighted avg       0.36      0.60      0.45       250\n",
      "\n",
      "New best Macro F1-Score: 0.1867. Saving model...\n",
      "Epoch 2/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.3660: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:42<00:00,  5.57it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 27.36it/s]\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7815\n",
      "Validation Accuracy: 0.7040\n",
      "Validation Macro F1-Score: 0.4402\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.99      0.83       149\n",
      "           1       0.54      0.58      0.56        24\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.88      0.24      0.37        63\n",
      "\n",
      "    accuracy                           0.70       250\n",
      "   macro avg       0.53      0.45      0.44       250\n",
      "weighted avg       0.70      0.70      0.64       250\n",
      "\n",
      "New best Macro F1-Score: 0.4402. Saving model...\n",
      "Epoch 3/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.3482: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:41<00:00,  5.67it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 26.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6260\n",
      "Validation Accuracy: 0.8640\n",
      "Validation Macro F1-Score: 0.8309\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92       149\n",
      "           1       0.94      0.71      0.81        24\n",
      "           2       1.00      0.71      0.83        14\n",
      "           3       0.75      0.78      0.77        63\n",
      "\n",
      "    accuracy                           0.86       250\n",
      "   macro avg       0.90      0.78      0.83       250\n",
      "weighted avg       0.87      0.86      0.86       250\n",
      "\n",
      "New best Macro F1-Score: 0.8309. Saving model...\n",
      "Epoch 4/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.4852: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:46<00:00,  5.12it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 25.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4297\n",
      "Validation Accuracy: 0.9080\n",
      "Validation Macro F1-Score: 0.8979\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94       149\n",
      "           1       1.00      0.83      0.91        24\n",
      "           2       0.93      0.93      0.93        14\n",
      "           3       0.92      0.73      0.81        63\n",
      "\n",
      "    accuracy                           0.91       250\n",
      "   macro avg       0.94      0.87      0.90       250\n",
      "weighted avg       0.91      0.91      0.90       250\n",
      "\n",
      "New best Macro F1-Score: 0.8979. Saving model...\n",
      "Epoch 5/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.3969: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:44<00:00,  5.38it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 27.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2482\n",
      "Validation Accuracy: 0.9480\n",
      "Validation Macro F1-Score: 0.9483\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       149\n",
      "           1       0.96      0.96      0.96        24\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       1.00      0.84      0.91        63\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.97      0.93      0.95       250\n",
      "weighted avg       0.95      0.95      0.95       250\n",
      "\n",
      "New best Macro F1-Score: 0.9483. Saving model...\n",
      "Epoch 6/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.2066: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:43<00:00,  5.51it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1318\n",
      "Validation Accuracy: 0.9520\n",
      "Validation Macro F1-Score: 0.9528\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       149\n",
      "           1       0.96      1.00      0.98        24\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       1.00      0.83      0.90        63\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.97      0.94      0.95       250\n",
      "weighted avg       0.96      0.95      0.95       250\n",
      "\n",
      "New best Macro F1-Score: 0.9528. Saving model...\n",
      "Epoch 7/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.0257: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:42<00:00,  5.62it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 27.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0801\n",
      "Validation Accuracy: 0.9560\n",
      "Validation Macro F1-Score: 0.9606\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       149\n",
      "           1       1.00      1.00      1.00        24\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       0.98      0.86      0.92        63\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.98      0.94      0.96       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "New best Macro F1-Score: 0.9606. Saving model...\n",
      "Epoch 8/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.0125: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:43<00:00,  5.44it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0444\n",
      "Validation Accuracy: 0.9480\n",
      "Validation Macro F1-Score: 0.9547\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       149\n",
      "           1       1.00      1.00      1.00        24\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       0.96      0.84      0.90        63\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.97      0.94      0.95       250\n",
      "weighted avg       0.95      0.95      0.95       250\n",
      "\n",
      "Epoch 9/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.0050: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:41<00:00,  5.69it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 27.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0407\n",
      "Validation Accuracy: 0.9560\n",
      "Validation Macro F1-Score: 0.9603\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       149\n",
      "           1       1.00      1.00      1.00        24\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       1.00      0.84      0.91        63\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.98      0.94      0.96       250\n",
      "weighted avg       0.96      0.96      0.95       250\n",
      "\n",
      "Epoch 10/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.0004: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:42<00:00,  5.66it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0311\n",
      "Validation Accuracy: 0.9520\n",
      "Validation Macro F1-Score: 0.9572\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       149\n",
      "           1       1.00      1.00      1.00        24\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       1.00      0.83      0.90        63\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.98      0.94      0.96       250\n",
      "weighted avg       0.96      0.95      0.95       250\n",
      "\n",
      "Epoch 11/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.0324: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:44<00:00,  5.40it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 28.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0344\n",
      "Validation Accuracy: 0.9520\n",
      "Validation Macro F1-Score: 0.9575\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       149\n",
      "           1       1.00      1.00      1.00        24\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       0.98      0.84      0.91        63\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.98      0.94      0.96       250\n",
      "weighted avg       0.95      0.95      0.95       250\n",
      "\n",
      "Epoch 12/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.0504: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:40<00:00,  5.88it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 28.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0186\n",
      "Validation Accuracy: 0.9520\n",
      "Validation Macro F1-Score: 0.9572\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       149\n",
      "           1       1.00      1.00      1.00        24\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       1.00      0.83      0.90        63\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.98      0.94      0.96       250\n",
      "weighted avg       0.96      0.95      0.95       250\n",
      "\n",
      "Epoch 13/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.0310: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:42<00:00,  5.56it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 26.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0191\n",
      "Validation Accuracy: 0.9520\n",
      "Validation Macro F1-Score: 0.9572\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       149\n",
      "           1       1.00      1.00      1.00        24\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       1.00      0.83      0.90        63\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.98      0.94      0.96       250\n",
      "weighted avg       0.96      0.95      0.95       250\n",
      "\n",
      "Best Macro F1-Score achieved: 0.9606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup, AutoModelForSequenceClassification, Adafactor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math, string\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"\")\n",
    "\n",
    "# Define if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# lstm_hidden_size: int = 128, lstm_layers: int = 1, dropout_prob: float = 0.3, lr=2e-5\n",
    "class BilstmXLMRobertaClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 xlm_model_name: str,\n",
    "                 num_labels: int,\n",
    "                 lstm_hidden_size: int = 512,\n",
    "                 lstm_layers: int = 3,\n",
    "                 dropout_prob: float = 0.5):\n",
    "        super(BilstmXLMRobertaClassifier, self).__init__()\n",
    "\n",
    "        # Load XLM-RoBERTa model\n",
    "        self.roberta = AutoModel.from_pretrained(xlm_model_name, cache_dir=\"/home/rohit/expt/dp_expt/codalab/NAACL-2025/20698/xlm_roberta/\")\n",
    "        #self.roberta = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        # Unfreeze layers if necessary\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # BiLSTM layer with multiple layers\n",
    "        self.bilstm = nn.LSTM(input_size=768,  # Embeddings from XLM-RoBERTa\n",
    "                              hidden_size=lstm_hidden_size,\n",
    "                              num_layers=lstm_layers,\n",
    "                              bidirectional=True,\n",
    "                              batch_first=True)\n",
    "\n",
    "        # Initialize LSTM weights\n",
    "        for name, param in self.bilstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)  # Xavier uniform initialization for input-hidden weights\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)  # Orthogonal initialization for hidden-hidden weights\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param)  # Initialize biases to zeros\n",
    "\n",
    "        # Attention mechanism after BiLSTM\n",
    "        self.attention = nn.Linear(lstm_hidden_size * 2, 1)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(lstm_hidden_size * 2)\n",
    "\n",
    "        # Classification layer\n",
    "        self.classifier = nn.Linear(lstm_hidden_size * 2, num_labels)  # Multiply by 2 for BiLSTM\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: torch.Tensor,\n",
    "                labels: torch.Tensor = None):\n",
    "\n",
    "        # Get embeddings from XLM-RoBERTa model\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #outputs = self.roberta.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state  # Get the hidden states (XLM-RoBERTa output)\n",
    "\n",
    "\n",
    "        # BiLSTM layer\n",
    "        lstm_output, _ = self.bilstm(embeddings)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.tanh(self.attention(lstm_output))\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        lstm_output = torch.sum(lstm_output * attention_weights, dim=1)\n",
    "\n",
    "        # # Residual Normalize LSTM output\n",
    "        # lstm_output = lstm_output+self.layer_norm(lstm_output)\n",
    "\n",
    "        # # Residual Dropout\n",
    "        # lstm_output = lstm_output+self.dropout(lstm_output)\n",
    "\n",
    "        # Normalize LSTM output\n",
    "        lstm_output = self.layer_norm(lstm_output)\n",
    "\n",
    "        # Dropout\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "\n",
    "        # Classification layer\n",
    "        logits = self.classifier(lstm_output)\n",
    "\n",
    "        # Calculate loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bytesizedllm/MalayalamXLM_Roberta\", cache_dir=\"/home/rohit/expt/dp_expt/codalab/NAACL-2025/20698/xlm_roberta/\")\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # This regex pattern matches most emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "        \"\\U00002500-\\U00002BEF\"  # Chinese characters\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U00002702-\\U000027B0\"  # Additional symbols\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols and pictographs\n",
    "        \"\\U0001F910-\\U0001F9FF\"  # Supplemental symbols and pictographs continued\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', ' ', text)\n",
    "\n",
    "punctuation_list = [r'\\.', r',', r'\\?', r'!', r':', r';', r'\"', r'\\-', r'–', r'\\(', r'\\)', r'\\[', r'\\]', r'\\{', r'\\}', r'\\.\\.\\.', r'\\/', r'\\\\', r'@', r'&', r'\\*', r'#', r'%', r'_', r'~', r'`', r'\\^', r'\\|', r'=', r'<', r'>', r'\\+']\n",
    "# Create the regex pattern to match any of the punctuation marks\n",
    "punctuation_pattern = r'(' + '|'.join(punctuation_list) + r')'\n",
    "\n",
    "# Function to clean and tokenize input text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).replace(\"&amp;\",\" \").replace(\"<br>\",\" \").replace(\"&#39;\",\"'\")\n",
    "    text = remove_html_tags(text)\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "declabels = {'MOSTLY False news': 'MOSTLY FALSE',\n",
    "'HALF TRUE':'HALF TRUE',\n",
    "'Half true':'HALF TRUE',\n",
    "'PARTLY False news':'PARTLY FALSE',\n",
    "'Partly False news':'PARTLY FALSE',\n",
    "'False news':'FALSE',\n",
    "'Mostly False news':'MOSTLY FALSE'}\n",
    "\n",
    "# Data preparation function\n",
    "def load_and_clean_data(trainpath, validpath):\n",
    "    traindf = pd.read_csv(trainpath)\n",
    "    train_sents = []\n",
    "    train_labels = []\n",
    "    labels = list(set([l.strip() for l in traindf[\"Label\"]]))\n",
    "    label2id = {labels[i]:i for i in range(len(labels))}\n",
    "    for text1, label in zip( traindf[\"News\"], traindf[\"Label\"]):\n",
    "        label = label.strip()\n",
    "        train_sents.append(preprocess_text(text1))\n",
    "        train_labels.append(label2id[label])  # Label as int for multi-class\n",
    "\n",
    "    validdf = pd.read_csv(validpath)\n",
    "\n",
    "    valid_sents = []\n",
    "    valid_labels = []\n",
    "    for sent, label in zip(validdf[\"News\"], validdf[\"FactCheck\"]):\n",
    "        label = label.strip()\n",
    "        valid_sents.append(preprocess_text(sent))\n",
    "        valid_labels.append(label2id[declabels[label]])  # Label as int\n",
    "\n",
    "    return train_sents, train_labels, valid_sents, valid_labels, label2id\n",
    "\n",
    "# Load and clean data (assumed pre-written)\n",
    "train_texts, train_labels, val_texts, val_labels, label2id = load_and_clean_data(\"fake_news_classification_mal_train1.csv\", \"./old_data/FakeNews - Test Dataset.csv\")\n",
    "\n",
    "# Dataset class for PyTorch DataLoader\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize and encode the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset = HateSpeechDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = HateSpeechDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"bytesizedllm/MalayalamXLM_Roberta\"\n",
    "num_labels = len(list(set(train_labels)))\n",
    "print(\"No. of labels: \", num_labels, list(set(train_labels)), list(set(val_labels)), label2id)\n",
    "model = BilstmXLMRobertaClassifier(model_name, num_labels).to(device)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 13\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "# Training and Evaluation Functions\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(dataloader, leave=True, desc=\"Training\")\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        loss, logits = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()  # Move scheduler step inside the loop\n",
    "\n",
    "        # Gradient clipping\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_description(f\"Training Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    loop = tqdm(dataloader, leave=True, desc=\"Evaluating\")\n",
    "    with torch.no_grad():\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels)\n",
    "\n",
    "    return accuracy_score(true_labels, predictions), true_labels, predictions\n",
    "\n",
    "print(\"Train text:\", train_texts[3])\n",
    "print(\"val text:\", val_texts[10])\n",
    "\n",
    "# # Main Training Loop\n",
    "best_macro_f1 = 0.0\n",
    "best_model_path = \"best_model.pth\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    accuracy, true_labels, predictions = eval_model(model, val_loader, device)\n",
    "\n",
    "    report = classification_report(true_labels, predictions)\n",
    "    report1 = classification_report(true_labels, predictions, output_dict=True)\n",
    "    macro_f1 = report1['macro avg']['f1-score']\n",
    "\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation Macro F1-Score: {macro_f1:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Save best model\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best Macro F1-Score: {best_macro_f1:.4f}. Saving model...\")\n",
    "\n",
    "print(f\"Best Macro F1-Score achieved: {best_macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2cc103",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19469e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at bytesizedllm/MalayalamXLM_Roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Macro F1-Score: 0.9543\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        14\n",
      "           1       0.93      0.99      0.96       149\n",
      "           2       0.96      1.00      0.98        24\n",
      "           3       1.00      0.84      0.91        63\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.97      0.94      0.95       250\n",
      "weighted avg       0.95      0.95      0.95       250\n",
      "\n",
      "Predictions saved to prediction.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"bytesizedllm/MalayalamXLM_Roberta\"\n",
    "num_labels = len(label2id)\n",
    "label2id1 = {'PARTLY FALSE': 0, 'FALSE': 1, 'HALF TRUE': 2, 'MOSTLY FALSE': 3} #0.9543 best_model.pth\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bytesizedllm/MalayalamXLM_Roberta\", cache_dir=\"/home/rohit/expt/dp_expt/codalab/NAACL-2025/20698/xlm_roberta/\")\n",
    "\n",
    "\n",
    "model = BilstmXLMRobertaClassifier(model_name, num_labels)\n",
    "best_model_path = \"best_model.pth\" # label2id (1e-5) {'HALF TRUE': 0, 'FALSE': 1, 'PARTLY FALSE': 2, 'MOSTLY FALSE': 3} Validation Macro F1-Score: 0.9511\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_label(text):\n",
    "    with torch.no_grad():\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move input to device\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pred = torch.argmax(logits, dim=1).cpu().item()  # Get the predicted label\n",
    "    return pred\n",
    "\n",
    "# Load the test dataset\n",
    "test_path = \"old_data/FakeNews - Test Dataset.csv\"  # Path to the test dataset\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "true_predictions = []\n",
    "predicted = []\n",
    "for text, label in zip(test_df[\"News\"],test_df[\"FactCheck\"]):\n",
    "    pred = predict_label(preprocess_text(text))\n",
    "    predicted.append(pred)\n",
    "    true_predictions.append(label2id1[declabels[label]])\n",
    "\n",
    "\n",
    "report = classification_report(true_predictions, predicted)\n",
    "report1 = classification_report(true_predictions, predicted, output_dict=True)\n",
    "macro_f1 = report1['macro avg']['f1-score']\n",
    "\n",
    "print(f\"Validation Macro F1-Score: {macro_f1:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "# output_df.to_csv(\"prediction.csv\", index=False)\n",
    "\n",
    "# print(\"Predictions saved to prediction.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_path = \"fake_news_classification_mal_test.csv\"  # Path to the test dataset\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "id2label = {v:k for k, v in label2id1.items()}\n",
    "\n",
    "data = []\n",
    "for text, ID in zip(test_df[\"News\"],test_df[\"S.no\"]):\n",
    "    pred = predict_label(preprocess_text(text))\n",
    "    pred = id2label[pred]\n",
    "    data.append([ID, pred])\n",
    "\n",
    "output_df = pd.DataFrame(data, columns = [\"S.no\", \"Label\"])\n",
    "    \n",
    "\n",
    "    \n",
    "output_df.to_csv(\"ByteSizedLLM_Malayalam_task2_run1.tsv\", sep = \"\\t\", index=False)\n",
    "\n",
    "print(\"Predictions saved to prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0941de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55026916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 13:11:15.834787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/rohit/.cache/huggingface/token\n",
      "Login successful\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HALF TRUE': 0, 'PARTLY FALSE': 1, 'FALSE': 2, 'MOSTLY FALSE': 3}\n",
      "No. of labels:  4 [0, 1, 2, 3] [0, 1, 2, 3] {'HALF TRUE': 0, 'PARTLY FALSE': 1, 'FALSE': 2, 'MOSTLY FALSE': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at bytesizedllm/MalayalamXLM_Roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train text: വിഴിഞ്ഞത്ത് തീരദേശവാസികള്‍ ആക്രമിച്ചപ്പോള്‍ മുഖ്യമന്ത്രി പിണറായി വിജയന്‍ ഉപേക്ഷിച്ച കാര്‍.\n",
      "val text: സ്നേഹത്തില്‍ പൊതിഞ്ഞ പാവക്കുട്ടികള്‍, ഈ സമ്മാനമെല്ലാം പലസ്‌തീനിലെ കുട്ടികള്‍ക്കോ?\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.7169: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:15<00:00,  3.91it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 16.01it/s]\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9114\n",
      "Validation Accuracy: 0.5960\n",
      "Validation Macro F1-Score: 0.1867\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        24\n",
      "           1       0.00      0.00      0.00        14\n",
      "           2       0.60      1.00      0.75       149\n",
      "           3       0.00      0.00      0.00        63\n",
      "\n",
      "    accuracy                           0.60       250\n",
      "   macro avg       0.15      0.25      0.19       250\n",
      "weighted avg       0.36      0.60      0.45       250\n",
      "\n",
      "New best Macro F1-Score: 0.1867. Saving model...\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.3788: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:13<00:00,  4.39it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 16.01it/s]\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8044\n",
      "Validation Accuracy: 0.6880\n",
      "Validation Macro F1-Score: 0.3318\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        24\n",
      "           1       0.00      0.00      0.00        14\n",
      "           2       0.71      0.97      0.82       149\n",
      "           3       0.60      0.44      0.51        63\n",
      "\n",
      "    accuracy                           0.69       250\n",
      "   macro avg       0.33      0.35      0.33       250\n",
      "weighted avg       0.57      0.69      0.62       250\n",
      "\n",
      "New best Macro F1-Score: 0.3318. Saving model...\n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 1.0176: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:13<00:00,  4.52it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 16.36it/s]\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohit/anaconda3/envs/digi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7478\n",
      "Validation Accuracy: 0.7120\n",
      "Validation Macro F1-Score: 0.3852\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.04      0.08        24\n",
      "           1       0.00      0.00      0.00        14\n",
      "           2       0.81      0.87      0.84       149\n",
      "           3       0.53      0.75      0.62        63\n",
      "\n",
      "    accuracy                           0.71       250\n",
      "   macro avg       0.46      0.42      0.39       250\n",
      "weighted avg       0.67      0.71      0.67       250\n",
      "\n",
      "New best Macro F1-Score: 0.3852. Saving model...\n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.8498: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:13<00:00,  4.31it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6437\n",
      "Validation Accuracy: 0.7680\n",
      "Validation Macro F1-Score: 0.5396\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.25      0.35        24\n",
      "           1       1.00      0.14      0.25        14\n",
      "           2       0.84      0.93      0.88       149\n",
      "           3       0.63      0.73      0.68        63\n",
      "\n",
      "    accuracy                           0.77       250\n",
      "   macro avg       0.77      0.51      0.54       250\n",
      "weighted avg       0.77      0.77      0.74       250\n",
      "\n",
      "New best Macro F1-Score: 0.5396. Saving model...\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.4160: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:15<00:00,  3.84it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5335\n",
      "Validation Accuracy: 0.8160\n",
      "Validation Macro F1-Score: 0.6869\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.67      0.63        24\n",
      "           1       0.83      0.36      0.50        14\n",
      "           2       0.91      0.93      0.92       149\n",
      "           3       0.69      0.71      0.70        63\n",
      "\n",
      "    accuracy                           0.82       250\n",
      "   macro avg       0.76      0.67      0.69       250\n",
      "weighted avg       0.82      0.82      0.81       250\n",
      "\n",
      "New best Macro F1-Score: 0.6869. Saving model...\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.9172: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:14<00:00,  4.08it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4366\n",
      "Validation Accuracy: 0.8600\n",
      "Validation Macro F1-Score: 0.7899\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.96      0.69        24\n",
      "           1       0.91      0.71      0.80        14\n",
      "           2       0.92      0.97      0.95       149\n",
      "           3       0.95      0.59      0.73        63\n",
      "\n",
      "    accuracy                           0.86       250\n",
      "   macro avg       0.83      0.81      0.79       250\n",
      "weighted avg       0.89      0.86      0.86       250\n",
      "\n",
      "New best Macro F1-Score: 0.7899. Saving model...\n",
      "Epoch 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.1149: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:15<00:00,  3.87it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 15.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3349\n",
      "Validation Accuracy: 0.8880\n",
      "Validation Macro F1-Score: 0.8508\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.96      0.77        24\n",
      "           1       1.00      0.86      0.92        14\n",
      "           2       0.91      0.99      0.95       149\n",
      "           3       1.00      0.62      0.76        63\n",
      "\n",
      "    accuracy                           0.89       250\n",
      "   macro avg       0.89      0.86      0.85       250\n",
      "weighted avg       0.91      0.89      0.88       250\n",
      "\n",
      "New best Macro F1-Score: 0.8508. Saving model...\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.4656: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:14<00:00,  4.26it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 15.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2440\n",
      "Validation Accuracy: 0.8960\n",
      "Validation Macro F1-Score: 0.8527\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85        24\n",
      "           1       0.91      0.71      0.80        14\n",
      "           2       0.93      0.96      0.94       149\n",
      "           3       0.82      0.81      0.82        63\n",
      "\n",
      "    accuracy                           0.90       250\n",
      "   macro avg       0.88      0.83      0.85       250\n",
      "weighted avg       0.90      0.90      0.89       250\n",
      "\n",
      "New best Macro F1-Score: 0.8527. Saving model...\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.0851: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:14<00:00,  4.14it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 15.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1912\n",
      "Validation Accuracy: 0.9360\n",
      "Validation Macro F1-Score: 0.9322\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94        24\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.93      0.99      0.96       149\n",
      "           3       0.94      0.81      0.87        63\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.96      0.91      0.93       250\n",
      "weighted avg       0.94      0.94      0.93       250\n",
      "\n",
      "New best Macro F1-Score: 0.9322. Saving model...\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.1032: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:13<00:00,  4.41it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1461\n",
      "Validation Accuracy: 0.9360\n",
      "Validation Macro F1-Score: 0.9376\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        24\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.91      0.99      0.95       149\n",
      "           3       0.98      0.79      0.88        63\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.96      0.92      0.94       250\n",
      "weighted avg       0.94      0.94      0.93       250\n",
      "\n",
      "New best Macro F1-Score: 0.9376. Saving model...\n",
      "Epoch 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.3123: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:13<00:00,  4.32it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1294\n",
      "Validation Accuracy: 0.9360\n",
      "Validation Macro F1-Score: 0.9300\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        24\n",
      "           1       1.00      0.86      0.92        14\n",
      "           2       0.91      0.99      0.95       149\n",
      "           3       0.98      0.81      0.89        63\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.96      0.90      0.93       250\n",
      "weighted avg       0.94      0.94      0.93       250\n",
      "\n",
      "Epoch 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch Loss: 0.1056: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:14<00:00,  4.17it/s]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 14.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1165\n",
      "Validation Accuracy: 0.9360\n",
      "Validation Macro F1-Score: 0.9333\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94        24\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      0.99      0.95       149\n",
      "           3       0.96      0.81      0.88        63\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.96      0.91      0.93       250\n",
      "weighted avg       0.94      0.94      0.93       250\n",
      "\n",
      "Best Macro F1-Score achieved: 0.9376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup, AutoModelForSequenceClassification, Adafactor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math, string\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "# Define if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class TransformerXLMRobertaClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 xlm_model_name: str,\n",
    "                 num_labels: int,\n",
    "                 d_model: int = 768,\n",
    "                 nhead: int = 8,\n",
    "                 num_encoder_layers: int = 3,\n",
    "                 num_decoder_layers: int = 3,\n",
    "                 dim_feedforward: int = 2048,\n",
    "                 dropout_prob: float = 0.5):\n",
    "        super(TransformerXLMRobertaClassifier, self).__init__()\n",
    "\n",
    "        # Load XLM-RoBERTa model\n",
    "        self.roberta = AutoModel.from_pretrained(xlm_model_name, cache_dir=\"/home/rohit/expt/dp_expt/codalab/NAACL-2025/20698/xlm_roberta/\")\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, \n",
    "                                                   nhead=nhead, \n",
    "                                                   dim_feedforward=dim_feedforward, \n",
    "                                                   dropout=dropout_prob)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Transformer Decoder Layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, \n",
    "                                                   nhead=nhead, \n",
    "                                                   dim_feedforward=dim_feedforward, \n",
    "                                                   dropout=dropout_prob)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        # Linear layer to map XLM-RoBERTa output to transformer dimension\n",
    "        self.input_projection = nn.Linear(768, d_model)\n",
    "\n",
    "        # Output Classification Layer\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        self.gradient_clip_val = 1.0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get embeddings from XLM-RoBERTa\n",
    "        roberta_outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = roberta_outputs.last_hidden_state\n",
    "\n",
    "        # Project embeddings to match Transformer dimensions\n",
    "        embeddings = self.input_projection(embeddings)\n",
    "\n",
    "        # Create a source mask for the Transformer\n",
    "        seq_len = embeddings.size(1)\n",
    "        src_mask = self._generate_square_subsequent_mask(seq_len).to(embeddings.device)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_output = self.encoder(embeddings.permute(1, 0, 2), src_mask)\n",
    "\n",
    "        # Dummy target input for the Transformer Decoder\n",
    "        # Here, we use the same encoder output for simplicity\n",
    "        tgt = encoder_output.clone()\n",
    "        tgt_mask = self._generate_square_subsequent_mask(seq_len).to(encoder_output.device)\n",
    "\n",
    "        # Transformer Decoder\n",
    "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
    "\n",
    "        # Take the output of the last token\n",
    "        output = decoder_output.permute(1, 0, 2).mean(dim=1)\n",
    "\n",
    "        # Dropout and Classification\n",
    "        output = self.dropout(output)\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        # Calculate loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        \"\"\"Generate a square mask for the sequence to prevent attention to future tokens.\"\"\"\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
    "        return mask\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bytesizedllm/MalayalamXLM_Roberta\", cache_dir=\"/home/rohit/expt/dp_expt/codalab/NAACL-2025/20698/xlm_roberta/\")\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # This regex pattern matches most emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "        \"\\U00002500-\\U00002BEF\"  # Chinese characters\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U00002702-\\U000027B0\"  # Additional symbols\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols and pictographs\n",
    "        \"\\U0001F910-\\U0001F9FF\"  # Supplemental symbols and pictographs continued\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', ' ', text)\n",
    "\n",
    "punctuation_list = [r'\\.', r',', r'\\?', r'!', r':', r';', r'\"', r'\\-', r'–', r'\\(', r'\\)', r'\\[', r'\\]', r'\\{', r'\\}', r'\\.\\.\\.', r'\\/', r'\\\\', r'@', r'&', r'\\*', r'#', r'%', r'_', r'~', r'`', r'\\^', r'\\|', r'=', r'<', r'>', r'\\+']\n",
    "# Create the regex pattern to match any of the punctuation marks\n",
    "punctuation_pattern = r'(' + '|'.join(punctuation_list) + r')'\n",
    "\n",
    "# Function to clean and tokenize input text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).replace(\"&amp;\",\" \").replace(\"<br>\",\" \").replace(\"&#39;\",\"'\")\n",
    "    text = remove_html_tags(text)\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "declabels = {'MOSTLY False news': 'MOSTLY FALSE',\n",
    "'HALF TRUE':'HALF TRUE',\n",
    "'Half true':'HALF TRUE',\n",
    "'PARTLY False news':'PARTLY FALSE',\n",
    "'Partly False news':'PARTLY FALSE',\n",
    "'False news':'FALSE',\n",
    "'Mostly False news':'MOSTLY FALSE'}\n",
    "\n",
    "# Data preparation function\n",
    "def load_and_clean_data(trainpath, validpath):\n",
    "    traindf = pd.read_csv(trainpath)\n",
    "    train_sents = []\n",
    "    train_labels = []\n",
    "    labels = list(set([l.strip() for l in traindf[\"Label\"]]))\n",
    "    label2id = {labels[i]:i for i in range(len(labels))}\n",
    "    for text1, label in zip( traindf[\"News\"], traindf[\"Label\"]):\n",
    "        label = label.strip()\n",
    "        train_sents.append(preprocess_text(text1))\n",
    "        train_labels.append(label2id[label])  # Label as int for multi-class\n",
    "\n",
    "    validdf = pd.read_csv(validpath)\n",
    "\n",
    "    valid_sents = []\n",
    "    valid_labels = []\n",
    "    for sent, label in zip(validdf[\"News\"], validdf[\"FactCheck\"]):\n",
    "        label = label.strip()\n",
    "        valid_sents.append(preprocess_text(sent))\n",
    "        valid_labels.append(label2id[declabels[label]])  # Label as int\n",
    "\n",
    "    return train_sents, train_labels, valid_sents, valid_labels, label2id\n",
    "\n",
    "# Load and clean data (assumed pre-written)\n",
    "train_texts, train_labels, val_texts, val_labels, label2id = load_and_clean_data(\"fake_news_classification_mal_train1.csv\", \"./old_data/FakeNews - Test Dataset.csv\")\n",
    "\n",
    "print(label2id)\n",
    "\n",
    "# Dataset class for PyTorch DataLoader\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize and encode the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset = HateSpeechDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = HateSpeechDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"bytesizedllm/MalayalamXLM_Roberta\"\n",
    "num_labels = len(list(set(train_labels)))\n",
    "print(\"No. of labels: \", num_labels, list(set(train_labels)), list(set(val_labels)), label2id)\n",
    "model = TransformerXLMRobertaClassifier(model_name, num_labels).to(device)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 12\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "# Training and Evaluation Functions\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(dataloader, leave=True, desc=\"Training\")\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        loss, logits = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()  # Move scheduler step inside the loop\n",
    "\n",
    "        # Gradient clipping\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_description(f\"Training Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    loop = tqdm(dataloader, leave=True, desc=\"Evaluating\")\n",
    "    with torch.no_grad():\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels)\n",
    "\n",
    "    return accuracy_score(true_labels, predictions), true_labels, predictions\n",
    "\n",
    "print(\"Train text:\", train_texts[3])\n",
    "print(\"val text:\", val_texts[10])\n",
    "\n",
    "# # Main Training Loop\n",
    "best_macro_f1 = 0.0\n",
    "best_model_path = \"best_model2.pth\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    accuracy, true_labels, predictions = eval_model(model, val_loader, device)\n",
    "\n",
    "    report = classification_report(true_labels, predictions)\n",
    "    report1 = classification_report(true_labels, predictions, output_dict=True)\n",
    "    macro_f1 = report1['macro avg']['f1-score']\n",
    "\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation Macro F1-Score: {macro_f1:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Save best model\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best Macro F1-Score: {best_macro_f1:.4f}. Saving model...\")\n",
    "\n",
    "print(f\"Best Macro F1-Score achieved: {best_macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb60d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digi",
   "language": "python",
   "name": "digi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
